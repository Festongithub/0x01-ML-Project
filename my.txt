Using: cpu
NeuralNetwork(
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (linear_relu_stack): Sequential(
    (0): Linear(in_features=784, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=512, bias=True)
    (3): ReLU()
    (4): Linear(in_features=512, out_features=10, bias=True)
  )
)
Predicted class:tensor([4])
torch.Size([3, 28, 28])
torch.Size([3, 784])
torch.Size([3, 20])
Before Relu: tensor([[ 0.2601, -0.3040, -0.7160, -0.3222,  0.0287,  0.1553,  0.1206, -0.0962,
          0.1895,  0.3495,  0.3619, -0.9044,  0.7256, -0.2140,  0.6231, -0.2989,
          0.1026,  0.1043,  0.3049, -0.2455],
        [ 0.4042,  0.0858, -0.6386, -0.4328,  0.2739,  0.0708, -0.3627,  0.1642,
          0.1979,  0.6400,  0.3809, -0.5940,  0.2216, -0.0730,  0.4276,  0.1457,
         -0.3835, -0.1304,  0.5308, -0.3821],
        [ 0.2232, -0.2441, -0.7566, -0.5111,  0.4889,  0.0361, -0.4212,  0.0734,
         -0.0367,  0.2827,  0.2492, -0.4333,  0.4232, -0.0582,  0.3012, -0.0251,
         -0.2298,  0.1116,  0.0736, -0.5007]], grad_fn=<AddmmBackward0>)
After Relu: tensor([[0.2601, 0.0000, 0.0000, 0.0000, 0.0287, 0.1553, 0.1206, 0.0000, 0.1895,
         0.3495, 0.3619, 0.0000, 0.7256, 0.0000, 0.6231, 0.0000, 0.1026, 0.1043,
         0.3049, 0.0000],
        [0.4042, 0.0858, 0.0000, 0.0000, 0.2739, 0.0708, 0.0000, 0.1642, 0.1979,
         0.6400, 0.3809, 0.0000, 0.2216, 0.0000, 0.4276, 0.1457, 0.0000, 0.0000,
         0.5308, 0.0000],
        [0.2232, 0.0000, 0.0000, 0.0000, 0.4889, 0.0361, 0.0000, 0.0734, 0.0000,
         0.2827, 0.2492, 0.0000, 0.4232, 0.0000, 0.3012, 0.0000, 0.0000, 0.1116,
         0.0736, 0.0000]], grad_fn=<ReluBackward0>)
Using: cpu
NeuralNetwork(
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (linear_relu_stack): Sequential(
    (0): Linear(in_features=784, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=512, bias=True)
    (3): ReLU()
    (4): Linear(in_features=512, out_features=10, bias=True)
  )
)
Predicted class:tensor([0])
torch.Size([3, 28, 28])
torch.Size([3, 784])
torch.Size([3, 20])
Before Relu: tensor([[ 0.2156, -0.1128, -0.4541, -0.2038, -0.3887,  0.2760, -0.6310,  0.3416,
          0.3893, -0.5700, -0.1567,  0.2509, -0.0055, -0.2291, -0.0900, -0.1881,
         -0.1349,  0.4593, -0.0431,  0.3318],
        [-0.0320, -0.2612, -0.8594,  0.0031, -0.0819,  0.2627, -0.3277,  0.2316,
          0.2750, -0.3901, -0.2482,  0.4605,  0.3493, -0.1738, -0.2223, -0.1430,
         -0.3566,  0.3101,  0.1946,  0.3553],
        [ 0.1489,  0.1947, -0.7674,  0.2640, -0.5277,  0.3217, -0.3894,  0.4288,
          0.5144, -0.1621, -0.1811,  0.0253,  0.3182, -0.3306,  0.1226,  0.0367,
         -0.1733,  0.2322, -0.2127,  0.5133]], grad_fn=<AddmmBackward0>)
After Relu: tensor([[0.2156, 0.0000, 0.0000, 0.0000, 0.0000, 0.2760, 0.0000, 0.3416, 0.3893,
         0.0000, 0.0000, 0.2509, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4593,
         0.0000, 0.3318],
        [0.0000, 0.0000, 0.0000, 0.0031, 0.0000, 0.2627, 0.0000, 0.2316, 0.2750,
         0.0000, 0.0000, 0.4605, 0.3493, 0.0000, 0.0000, 0.0000, 0.0000, 0.3101,
         0.1946, 0.3553],
        [0.1489, 0.1947, 0.0000, 0.2640, 0.0000, 0.3217, 0.0000, 0.4288, 0.5144,
         0.0000, 0.0000, 0.0253, 0.3182, 0.0000, 0.1226, 0.0367, 0.0000, 0.2322,
         0.0000, 0.5133]], grad_fn=<ReluBackward0>)
Model structure:NeuralNetwork(
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (linear_relu_stack): Sequential(
    (0): Linear(in_features=784, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=512, bias=True)
    (3): ReLU()
    (4): Linear(in_features=512, out_features=10, bias=True)
  )
)
Layer:linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values: Parameter containing:
tensor([[-0.0294, -0.0347,  0.0280,  ..., -0.0229, -0.0027, -0.0179],
        [ 0.0034, -0.0231, -0.0092,  ..., -0.0338, -0.0173, -0.0011],
        [-0.0298,  0.0153, -0.0163,  ...,  0.0028, -0.0108, -0.0197],
        ...,
        [ 0.0216, -0.0092,  0.0045,  ..., -0.0167,  0.0236,  0.0300],
        [ 0.0246, -0.0251, -0.0287,  ..., -0.0206,  0.0304, -0.0232],
        [-0.0262,  0.0229,  0.0022,  ...,  0.0074, -0.0104, -0.0136]],
       requires_grad=True)
Layer:linear_relu_stack.0.bias | Size: torch.Size([512]) | Values: Parameter containing:
tensor([ 1.9548e-02, -3.3344e-02,  2.0731e-02, -3.1583e-02, -1.0722e-02,
        -1.0477e-03, -3.0671e-02, -2.4694e-03,  2.2262e-05,  2.6661e-02,
         9.6648e-03,  4.4136e-03,  2.3025e-02,  1.3916e-02, -2.3546e-03,
         2.7416e-02, -1.2625e-02, -3.2346e-02, -2.8109e-02, -3.4061e-02,
        -1.1436e-02, -3.6083e-03, -2.4776e-02, -1.6628e-02,  3.3781e-02,
         2.1074e-02,  2.2285e-02, -1.6689e-02,  9.9282e-03,  1.1447e-02,
         3.4703e-02,  3.2767e-02, -9.8306e-03,  2.9618e-02,  8.6612e-03,
        -3.1389e-02, -1.7909e-03, -6.7965e-03,  2.0476e-02,  2.2715e-02,
         1.7787e-03, -4.2798e-03,  2.0159e-02,  1.6641e-02, -1.0676e-02,
        -2.8585e-02, -2.5241e-04,  2.6810e-02, -1.9622e-02,  3.4675e-02,
        -3.1576e-02, -1.8885e-02,  2.7975e-02,  9.2736e-03, -2.3703e-02,
        -8.8189e-03, -1.4000e-02, -5.5851e-04,  2.0361e-02, -3.5633e-02,
         2.3567e-02, -2.5704e-02, -8.0148e-03, -3.3928e-02,  5.2966e-03,
         1.9480e-02,  2.5779e-02, -2.9663e-02, -9.1233e-03,  2.1607e-02,
         1.6158e-02, -2.4765e-02, -3.5037e-02,  1.4807e-02, -2.5889e-02,
         1.1786e-02,  3.4016e-02, -1.3886e-02, -2.8619e-02,  3.4580e-03,
         1.9757e-02,  4.3546e-03,  2.7839e-02,  2.9297e-02,  1.2645e-02,
         2.6076e-02, -1.3033e-03,  1.1972e-02, -1.4780e-03, -2.2618e-02,
        -2.7156e-02,  1.1455e-02,  1.6898e-02, -1.3109e-02, -1.9393e-02,
        -3.9795e-03, -1.4637e-03, -1.3146e-02, -2.6100e-02,  2.2453e-02,
         1.3075e-02,  3.1519e-02, -3.1337e-02,  1.1550e-03, -2.3033e-02,
         3.4693e-02,  1.1410e-02,  3.4740e-02, -1.8954e-02,  1.3750e-02,
         1.0493e-02,  1.6623e-03, -1.8154e-03,  6.3819e-03,  2.3017e-02,
        -3.1080e-02, -3.4886e-02, -4.3166e-03, -1.8549e-02, -1.3719e-02,
         3.3936e-03,  3.2282e-02, -1.1180e-02,  5.3038e-04, -1.2548e-02,
         7.0156e-03,  4.6363e-03,  2.4013e-02,  3.1436e-02, -2.2271e-02,
        -5.5427e-03, -5.9926e-03,  2.3520e-02,  1.3548e-02,  1.9910e-02,
         1.8602e-02,  6.2429e-03,  1.6349e-02,  1.4905e-02, -2.9662e-02,
         2.2736e-02,  3.1746e-02, -3.4390e-02, -2.3259e-02, -3.2463e-02,
         1.8133e-03, -2.9452e-02,  1.3104e-02,  1.2600e-02, -1.2799e-02,
        -9.0447e-03, -4.1750e-03,  9.2573e-03, -1.9920e-02, -3.3391e-02,
        -3.5592e-02,  1.2487e-03,  2.2487e-02, -2.8656e-02,  1.5345e-02,
        -2.0919e-02,  2.1577e-02, -2.9938e-04, -3.1409e-02, -1.5077e-02,
        -1.9306e-02,  2.4722e-02,  2.6673e-02,  2.7979e-03,  1.2503e-02,
        -6.1032e-03, -1.6848e-02,  2.4803e-02, -2.6995e-02, -3.0418e-02,
        -3.1391e-03,  4.1572e-03,  1.4184e-02, -2.4009e-02,  4.7725e-03,
         8.6407e-03, -3.0500e-02, -4.2969e-03,  1.6814e-02, -9.3059e-03,
        -5.6017e-03,  3.4121e-02, -2.6439e-02,  1.0197e-02, -2.6067e-02,
        -2.6658e-02, -1.0347e-02, -6.6261e-03, -2.5482e-02,  9.5851e-03,
        -2.8764e-02,  1.4013e-02,  1.0934e-02,  3.3819e-02, -3.8208e-03,
         1.3932e-02,  2.1415e-02, -7.8028e-03,  9.2753e-03,  3.1479e-02,
        -1.2498e-02, -1.1474e-02,  1.5028e-02,  1.0666e-02, -1.4001e-02,
        -1.2217e-03, -2.0134e-02, -2.1602e-02, -2.5689e-02,  2.9464e-03,
         1.4512e-03, -2.6043e-02, -2.0072e-02, -1.9698e-02,  2.9249e-02,
         2.1604e-02, -1.9865e-02,  1.9865e-02,  1.9478e-03,  3.9416e-03,
        -1.3306e-02,  2.1506e-02,  8.1194e-03,  2.4107e-02, -3.2879e-02,
         9.8346e-03,  2.1984e-02,  2.7285e-02, -3.4506e-02,  9.1579e-03,
         3.3954e-02, -3.3614e-02, -2.7332e-02,  3.4875e-02, -2.3605e-02,
         3.4365e-02,  2.6157e-02, -2.2880e-02,  3.1027e-02,  3.1137e-02,
         3.5293e-02,  1.7733e-02, -6.1326e-03, -3.2464e-02, -1.0774e-02,
        -3.2111e-03, -4.6564e-03,  2.1752e-02, -2.3878e-04,  2.6605e-02,
         2.6302e-02,  3.1344e-02,  2.7996e-02, -2.7179e-02, -1.6866e-04,
         3.6934e-03,  2.8681e-02, -2.7647e-02,  3.2825e-02, -7.5558e-03,
        -2.1321e-02,  1.9715e-02, -1.4080e-02,  2.2532e-02, -4.6532e-03,
         2.3788e-02, -3.4791e-04,  9.4375e-05,  1.5737e-02,  2.9576e-03,
        -2.1287e-04,  2.3189e-02, -2.2336e-02,  3.3299e-02,  2.1552e-02,
         2.6731e-02, -1.2947e-02, -3.5455e-02,  1.5865e-02, -1.7147e-02,
        -1.6333e-02,  2.9275e-02, -2.3743e-02,  1.7875e-02, -1.4760e-02,
         2.6007e-02,  1.9368e-02, -2.3414e-02,  2.4783e-02,  1.7943e-02,
        -1.7393e-02, -1.2455e-02,  2.2466e-02, -2.7131e-02,  1.8061e-02,
        -1.1009e-02,  1.9089e-02, -2.0353e-02,  7.4013e-03,  8.6351e-03,
         3.1566e-02,  2.5928e-02, -2.5920e-02, -1.5884e-02, -2.2212e-02,
        -1.0994e-02, -1.9454e-03, -1.7656e-02, -5.8038e-03,  1.4069e-02,
        -7.9527e-03,  9.2850e-03, -3.1279e-02, -1.8865e-02, -1.0645e-02,
        -1.7115e-02,  1.5140e-02,  2.9747e-02,  2.0291e-02, -1.1833e-02,
        -1.0884e-02,  3.4158e-02, -2.0383e-02, -1.4633e-02, -3.3639e-02,
         4.3504e-03, -1.8048e-03, -4.7498e-03, -6.3539e-03,  1.9907e-02,
        -2.7659e-02, -4.2200e-03,  2.0821e-02,  1.9950e-02, -2.2281e-02,
         1.7486e-02,  2.9101e-02,  9.0406e-03,  1.8922e-03,  5.6202e-03,
        -1.3440e-03,  4.9445e-04, -2.9302e-02,  1.7714e-02, -1.3303e-03,
         8.9709e-03,  1.2243e-02, -2.8882e-02,  2.9145e-02,  2.1114e-02,
         1.7317e-02, -1.5943e-02,  2.0494e-03,  1.3272e-02, -2.0943e-03,
         1.0970e-02, -2.8933e-02,  1.8572e-03,  4.1165e-03, -3.4418e-02,
         4.7052e-03,  3.0061e-02,  4.2595e-03,  3.0615e-02,  6.7269e-03,
        -3.9845e-03, -1.0659e-02, -2.5672e-02,  1.3756e-02, -2.8147e-02,
        -1.0365e-02,  7.3596e-03, -7.7908e-04, -2.3648e-02,  2.0159e-02,
        -1.2049e-02,  1.7254e-02,  4.1675e-03,  2.3724e-02, -3.3156e-02,
         2.5349e-02, -3.4763e-02, -8.1164e-03, -8.1268e-04,  1.8708e-02,
         8.4115e-03, -1.8707e-02, -2.5564e-02, -1.8696e-02,  8.6646e-03,
         3.2341e-02,  2.7001e-02,  1.8256e-02,  2.9363e-02, -1.9024e-02,
         1.0942e-02,  3.2442e-02, -7.6271e-03, -2.6580e-02,  2.7575e-02,
         1.1834e-02, -7.3952e-03,  2.1755e-02,  2.5303e-02,  3.0003e-02,
         2.6378e-02, -4.9042e-03,  1.7729e-02,  2.8778e-02,  2.6131e-02,
        -1.4681e-02, -3.4974e-02,  1.1440e-03,  4.2197e-03, -2.7420e-03,
        -1.0755e-02,  1.2371e-02, -8.5212e-03, -1.8785e-02,  1.1952e-02,
        -3.4156e-02, -3.1669e-02,  2.7226e-02,  2.7748e-02, -2.2883e-02,
        -5.7986e-03, -3.5703e-02, -3.2253e-02,  2.4934e-03,  1.2700e-02,
         3.3852e-02, -1.4756e-02, -9.8813e-04, -3.4989e-02, -7.1752e-03,
        -1.9698e-02,  3.2095e-02, -1.2252e-02, -1.0844e-02,  1.0633e-02,
         3.1474e-02, -5.5007e-03, -8.7015e-03, -1.1717e-02,  1.0091e-02,
         3.2791e-03, -2.1577e-02,  2.7363e-02,  4.3342e-03,  2.2799e-02,
         1.5869e-02, -4.8845e-03,  3.4124e-02,  1.2413e-02, -1.2631e-02,
         2.0435e-02, -3.4014e-02,  3.3785e-02, -8.9311e-03, -9.6693e-03,
        -1.1276e-02, -3.3611e-02, -1.1072e-02, -3.5157e-02, -9.1652e-03,
        -6.9385e-03, -5.3364e-03, -2.0444e-02,  1.2924e-02, -3.0523e-02,
        -2.2632e-02,  1.9266e-02, -8.7892e-03, -2.4149e-02, -5.7414e-03,
         2.6571e-03, -2.2166e-02, -4.2219e-03,  1.8451e-02,  1.9929e-02,
        -6.8954e-03,  1.9656e-02, -3.1904e-02,  1.3694e-02,  1.1689e-02,
        -2.6056e-02,  2.9561e-02,  9.4057e-03, -2.6451e-02, -9.4896e-04,
         4.5631e-03,  1.0420e-02,  5.6367e-03,  2.4947e-02,  3.5414e-02,
        -3.2817e-04, -8.6389e-04,  2.8744e-02,  9.4963e-03,  3.3275e-02,
        -1.4107e-02, -3.4986e-03,  1.5676e-02,  2.6883e-02,  2.1565e-02,
         3.1619e-02, -3.3821e-02], requires_grad=True)
Layer:linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values: Parameter containing:
tensor([[-0.0242,  0.0397, -0.0251,  ..., -0.0075, -0.0203,  0.0157],
        [ 0.0347, -0.0102,  0.0272,  ..., -0.0235,  0.0189, -0.0383],
        [ 0.0163,  0.0009,  0.0311,  ..., -0.0118, -0.0071, -0.0106],
        ...,
        [-0.0055, -0.0215,  0.0366,  ...,  0.0040, -0.0106,  0.0091],
        [ 0.0416,  0.0040, -0.0041,  ...,  0.0005,  0.0192, -0.0181],
        [-0.0333,  0.0298,  0.0149,  ...,  0.0380, -0.0437,  0.0438]],
       requires_grad=True)
Layer:linear_relu_stack.2.bias | Size: torch.Size([512]) | Values: Parameter containing:
tensor([-1.6472e-03, -3.9655e-02,  1.0310e-02,  3.8079e-02,  2.1195e-02,
         1.9922e-02,  3.7683e-02,  2.4135e-02, -1.0351e-02, -3.6059e-02,
         2.3961e-02,  3.0179e-02, -1.2188e-02, -2.4957e-02,  1.0437e-02,
        -3.3665e-02, -2.3354e-02, -1.6137e-02, -3.2339e-02,  3.5456e-02,
         2.7495e-02,  4.0198e-02,  2.8751e-02,  4.2296e-02,  2.7221e-02,
         2.1509e-02, -2.4122e-02,  3.5367e-02, -2.2232e-02, -1.0729e-02,
         1.9745e-02,  5.9959e-03, -1.5981e-02,  4.3267e-02, -3.9498e-02,
         1.6117e-02, -2.9129e-02, -4.0394e-02,  1.4755e-02, -2.1623e-02,
         2.9102e-02, -2.5996e-02, -3.5854e-02,  3.2857e-02,  2.9300e-02,
         2.4689e-02, -1.0406e-02,  9.5647e-03, -1.4206e-02, -1.2684e-03,
         2.4692e-02, -1.3296e-02,  1.3548e-02, -2.5482e-02, -2.5586e-02,
        -4.3136e-02,  2.4197e-02, -2.7368e-02,  4.0327e-02,  3.3579e-03,
        -8.7957e-04, -1.6888e-03,  9.2106e-03,  5.9195e-03, -2.0113e-02,
        -1.5708e-02, -3.0908e-02,  9.4355e-03, -3.4144e-02, -9.6522e-03,
         1.0889e-02,  2.5969e-02,  1.6110e-02, -2.2761e-02,  2.9106e-02,
        -2.4014e-02,  2.0203e-02, -1.7805e-04,  4.3332e-04, -1.9321e-02,
         4.3418e-02,  8.4509e-03, -2.2040e-02,  2.6052e-02,  2.9398e-02,
        -2.4333e-03,  6.2171e-03,  4.3905e-02, -5.4644e-03, -2.4049e-02,
        -1.2209e-02, -3.2660e-02,  1.6489e-02,  3.8918e-03,  3.7344e-02,
        -4.0734e-02,  2.3607e-02, -1.1901e-02,  2.2201e-02, -2.0062e-02,
        -3.6512e-02, -3.8829e-02,  3.6202e-02,  3.1290e-02,  1.6144e-02,
         3.7376e-02, -2.7059e-02, -2.4591e-02, -1.9679e-02, -1.7419e-02,
        -4.2535e-02, -7.7922e-03, -1.2866e-02,  1.4763e-02,  1.9591e-02,
         4.2857e-02,  2.6918e-02,  4.0060e-02, -2.6359e-02,  1.8649e-02,
        -7.0774e-04,  1.8745e-02, -6.9604e-04,  4.0394e-02, -1.1187e-03,
         2.6226e-03, -4.1229e-02, -3.0264e-02,  2.6815e-02,  4.0101e-03,
        -4.0869e-02, -3.4314e-02, -3.9004e-02, -1.0045e-02, -3.9559e-02,
        -1.9339e-02, -3.6247e-02,  4.0764e-02,  8.4315e-03, -3.4495e-03,
         2.0657e-02, -8.3135e-04, -2.5619e-03,  2.2088e-02, -8.7311e-03,
         1.7299e-02, -1.6399e-02,  3.2850e-03, -2.2485e-02, -4.0667e-03,
         1.5318e-02, -2.3617e-02,  1.3658e-02,  1.2859e-02,  1.2064e-02,
         3.1075e-02,  6.2608e-03, -3.0072e-02, -4.0479e-02, -2.7714e-02,
         4.4082e-02, -1.1422e-02,  1.6793e-02, -4.3220e-03, -1.0128e-02,
         2.6765e-02,  6.3663e-03, -1.7277e-02, -3.8506e-02, -4.0433e-02,
         1.3634e-02, -1.5232e-02, -2.4153e-02,  4.0257e-02, -2.8928e-02,
        -1.5752e-02,  3.2472e-02, -7.3337e-04,  1.2580e-02, -3.4853e-02,
        -1.0360e-03, -3.7846e-02, -7.4527e-03, -1.7023e-03, -1.3644e-02,
        -2.4873e-02,  3.1274e-03,  2.7649e-02, -2.3895e-02,  3.1125e-02,
         3.4870e-02,  1.9449e-02, -1.9654e-02, -2.3618e-02, -1.1584e-02,
         3.6065e-02, -3.5415e-02, -4.0023e-02, -4.2395e-04,  3.5489e-02,
         3.4166e-02,  5.1841e-04,  3.3971e-02,  1.7158e-02, -4.2101e-03,
         1.3820e-02, -4.1019e-02,  2.8843e-02, -1.5395e-02,  1.6387e-02,
        -3.2944e-02,  1.6121e-02, -1.1831e-02,  2.2645e-02, -1.9748e-02,
         1.3040e-02, -1.9528e-02,  3.7866e-02,  9.4411e-03, -1.3269e-02,
         2.9593e-02,  1.7162e-02, -2.3333e-02,  2.4616e-02,  4.2093e-02,
        -1.3978e-02,  3.0605e-02,  2.1474e-02, -1.1959e-03,  2.0039e-02,
         6.8154e-03, -8.7370e-03,  2.5914e-02, -3.2103e-02, -3.6251e-03,
         1.8255e-02,  3.6852e-03,  5.1921e-03, -2.4715e-03, -7.3794e-03,
         2.0603e-02,  2.1552e-02,  2.8828e-02,  3.6275e-02,  1.1729e-02,
        -4.0330e-02, -8.3817e-03,  1.5814e-03, -1.6323e-02,  2.1690e-03,
        -2.4605e-03, -1.1831e-02,  1.2732e-02, -2.9094e-02, -8.8770e-03,
        -1.7780e-02, -4.9118e-04,  1.5416e-02, -3.3225e-02,  4.1230e-03,
         9.3139e-04,  2.4313e-02, -3.8037e-02,  7.1480e-04, -2.6782e-02,
         3.4795e-02,  5.1553e-03, -2.4951e-02, -1.6981e-02, -8.3297e-03,
        -3.0149e-02, -1.4263e-02,  3.7460e-02,  1.2963e-02, -1.5298e-02,
        -3.9512e-02,  4.0914e-02,  6.7658e-03,  2.0028e-02, -1.5287e-02,
         1.7175e-02,  4.3660e-02, -9.1936e-03,  3.7341e-02,  7.4222e-04,
         2.7184e-02,  2.5531e-02, -6.5885e-03, -5.8344e-03,  3.4738e-02,
        -2.1895e-02, -3.5291e-02,  1.6857e-02,  4.0509e-02,  3.7099e-02,
         2.2886e-02,  7.3150e-03, -1.6367e-02,  3.4690e-02, -3.7604e-02,
        -3.4806e-02,  1.8533e-03,  9.7907e-03,  1.5745e-02,  3.8887e-02,
         4.1243e-02,  3.6313e-02,  1.0970e-02, -3.4347e-02,  3.6421e-02,
         3.6028e-02, -2.8744e-02, -1.6611e-02,  8.3684e-03, -1.4466e-02,
        -1.7756e-02,  3.8500e-02,  9.8077e-03,  1.7730e-02,  4.4152e-02,
         3.7596e-02,  3.5912e-02, -1.6541e-02, -1.9899e-02,  1.3307e-02,
         3.6396e-02,  1.1294e-02,  1.7284e-02, -2.9662e-02,  1.6189e-02,
        -1.3129e-02,  4.2586e-02, -4.2152e-02, -2.4742e-02, -1.1520e-02,
        -1.0498e-02, -2.5071e-02,  5.9276e-03, -1.2997e-02,  3.1594e-02,
         3.2826e-02, -2.8366e-02, -2.2373e-02, -3.7542e-02, -1.1055e-02,
         3.0259e-02, -4.4091e-03, -3.9457e-02,  4.0678e-02,  1.0340e-02,
        -2.5155e-02, -1.4788e-02,  1.4561e-02, -3.2101e-02, -3.1555e-02,
         2.3984e-02, -2.2622e-02, -1.7802e-02, -3.8602e-02, -1.5900e-02,
         2.9811e-02,  1.9619e-02, -1.3465e-02, -1.5380e-02, -2.2759e-02,
        -2.4697e-02,  1.3246e-03,  1.1662e-02,  3.1062e-02,  2.8728e-02,
         3.8466e-02, -1.4895e-02, -3.9508e-02,  2.9340e-02, -4.0452e-02,
         2.0060e-02,  9.3053e-03, -3.8324e-02, -3.8384e-02, -2.6986e-02,
         2.3213e-02,  1.9058e-02,  5.5269e-03,  1.1371e-02,  2.6947e-02,
        -3.2402e-02,  2.8881e-02,  1.7146e-02,  1.3675e-03,  4.1545e-03,
         3.4628e-02,  2.8220e-03, -1.0883e-02,  1.3427e-02,  3.9601e-02,
         2.2288e-02,  3.0595e-02,  3.6553e-02, -2.1696e-02, -2.6336e-02,
        -1.7974e-02, -9.6279e-03,  4.1516e-02, -6.3565e-03,  1.9889e-02,
         2.0231e-02,  3.9641e-02,  1.1479e-02, -1.1860e-02,  3.1302e-02,
        -2.2535e-02, -2.1928e-02,  2.1099e-02,  1.0086e-02, -1.0143e-02,
         9.0365e-04, -3.2932e-02, -3.6388e-03, -2.1371e-02,  9.6642e-03,
         4.3270e-02,  1.6857e-02,  2.3395e-02,  5.1153e-03, -1.6665e-02,
        -3.9543e-03,  1.7148e-02,  3.2356e-02,  3.0963e-02, -2.7056e-02,
         7.9663e-05,  1.2659e-02, -1.4539e-02, -2.7517e-02, -7.5371e-03,
         1.3258e-03, -3.6894e-02,  2.9363e-02, -1.1300e-03,  3.3252e-02,
        -1.8836e-02, -1.9537e-02,  2.5167e-02, -6.0710e-03, -2.4547e-03,
        -8.2319e-03, -1.8088e-02,  3.9707e-02, -3.3026e-02,  3.0260e-03,
         9.0711e-03, -2.5950e-02, -3.3276e-02, -4.1954e-03, -4.8472e-03,
        -2.0234e-02,  2.5975e-02,  2.3807e-02,  2.9660e-02, -4.3864e-02,
        -5.5361e-03,  1.6070e-02, -5.6669e-03,  2.6974e-02, -2.1521e-02,
        -3.8162e-02, -4.0236e-02,  5.0992e-03,  1.4198e-02,  7.2649e-03,
        -7.1751e-03,  4.6370e-04,  4.3817e-02, -2.0817e-02, -2.6695e-02,
         3.7799e-02, -8.2862e-03, -5.8833e-03,  4.0736e-02,  1.5039e-02,
         2.2879e-02, -1.9712e-02,  8.1244e-03, -3.0205e-03, -2.2466e-02,
         1.9567e-02, -3.7827e-02,  9.6555e-03,  1.1750e-02, -3.5127e-02,
        -3.1259e-02, -2.3021e-02, -4.3375e-02,  2.3846e-02, -2.2523e-02,
        -8.3056e-04, -3.0956e-02, -1.5374e-02, -2.0534e-02, -2.1261e-02,
        -2.1457e-04,  2.5134e-02, -3.4578e-02, -1.9422e-02, -2.0266e-02,
         3.2078e-02,  1.6002e-03, -1.1873e-03,  1.2641e-02,  1.4206e-02,
         3.6149e-02,  1.0864e-02], requires_grad=True)
Layer:linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values: Parameter containing:
tensor([[-0.0408,  0.0355, -0.0136,  ..., -0.0229, -0.0198, -0.0144],
        [-0.0005, -0.0330,  0.0329,  ...,  0.0285, -0.0115, -0.0189],
        [-0.0178, -0.0101,  0.0110,  ..., -0.0177, -0.0417, -0.0242],
        ...,
        [ 0.0290,  0.0056, -0.0275,  ...,  0.0345, -0.0317, -0.0405],
        [ 0.0175, -0.0254, -0.0304,  ..., -0.0345,  0.0266, -0.0158],
        [-0.0141,  0.0144, -0.0102,  ...,  0.0215, -0.0087,  0.0309]],
       requires_grad=True)
Layer:linear_relu_stack.4.bias | Size: torch.Size([10]) | Values: Parameter containing:
tensor([ 0.0113, -0.0027, -0.0221, -0.0425,  0.0299, -0.0273, -0.0193, -0.0403,
        -0.0177,  0.0035], requires_grad=True)
